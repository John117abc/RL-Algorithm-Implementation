{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 值函数方法",
   "id": "b64a822943337368"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import deque\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# 设备选择\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "\n",
    "# 定义神经网络\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim,hidden_dim ,action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)  # 输出动作概率\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.softmax(self.fc3(x), dim=-1)  # 输出概率分布\n",
    "\n",
    "\n",
    "class PolicyAgent:\n",
    "    def __init__(self,state_dim,hidden_dim,action_dim,lr = 0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "\n",
    "        # 初始化神经网络\n",
    "        self.model = PolicyNetwork(state_dim, hidden_dim, action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr = lr)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.memory = deque()\n",
    "\n",
    "    # 使用神经网络输出概率分布，再使用贪心策略选择action\n",
    "    def select_action(self,state):\n",
    "        state = torch.tensor(state,dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            predict = self.model(state)\n",
    "        action = np.random.choice(self.action_dim,p=predict.squeeze().numpy())\n",
    "        return action\n",
    "\n",
    "    # 存储每一次action后得state,action,reward\n",
    "    def save_memory(self,state,action,reward):\n",
    "        self.memory.append((state,action,reward))\n",
    "\n",
    "    def calculate_returns(self):\n",
    "        returns = []\n",
    "        G = 0.0\n",
    "        # 从后往前遍历\n",
    "        for state, action, reward in reversed(self.memory):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return returns\n",
    "\n",
    "    # 更新策略\n",
    "    def update_policy(self):\n",
    "        if len(self.memory) == 0:\n",
    "            return  # 防止空经验更新\n",
    "        states, actions, rewards = zip(*self.memory)\n",
    "        returns = self.calculate_returns()\n",
    "\n",
    "        states = torch.tensor(states,dtype=torch.float32)\n",
    "        actions = torch.tensor(actions,dtype=torch.long)\n",
    "        returns = torch.tensor(returns,dtype=torch.float32)\n",
    "\n",
    "        # 取每一次被选择的那个动作的概率\n",
    "        choose = self.model(states)[torch.arange(len(states)), actions]\n",
    "        log = torch.log(choose + 1e-8)\n",
    "\n",
    "        # 定义损失函数，因为要做梯度上升，所以加上负号\n",
    "        loss_func = -(log * returns).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_func.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size, obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 训练循环\n",
    "global_step = 0\n",
    "episode_rewards = []\n",
    "\n",
    "\n",
    "# 使用策略梯度法进行训练\n",
    "agent = PolicyAgent(state_dim=2,hidden_dim=64,action_dim=4,lr = 0.0001)\n",
    "for episode in range(10000):\n",
    "    # 重置环境\n",
    "    observation, _ = env.reset(seed=99,options={'enable_random_pos': True})\n",
    "    state = observation['agent'] / (grid_world_size - 1 + 1e-8)  # 归一化到[0,1]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < 1000:  # 最大步数限制\n",
    "        # 选择动作\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # 执行动作\n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = next_observation['agent'] / (grid_world_size - 1 + 1e-8)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 存储经验\n",
    "        agent.save_memory(state,action,reward)\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        global_step += 1\n",
    "        step_count += 1\n",
    "\n",
    "    agent.update_policy()\n",
    "    agent.memory.clear()\n",
    "    # 更新探索率\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    # 打印进度\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f\"回合 {episode}, 平均奖励: {avg_reward:.2f}, 步数: {global_step}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(agent.model.state_dict(), '../data/value-function-method/vf-model.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 加载模型查看结果",
   "id": "c0033769ed08a041"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 5\n",
    "obstacle_count = 5\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human', obstacle_count=obstacle_count)\n",
    "\n",
    "\n",
    "# 使用策略梯度法进行训练\n",
    "agent = PolicyAgent(state_dim=2,hidden_dim=64,action_dim=4)\n",
    "\n",
    "# 测试学习到的策略\n",
    "print(\"\\n正在用学习到的策略运行测试 episode...\")\n",
    "observation, _ = test_env.reset(seed=99,options={'enable_random_pos': True})\n",
    "state = observation['agent'] / grid_world_size\n",
    "done = False\n",
    "total_reward = 0\n",
    "# 设备选择\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载模型参数\n",
    "model_path = '../data/value-function-method/vf-model.pth'\n",
    "print(f\"正在从 {model_path} 加载模型...\")\n",
    "params = torch.load(model_path, map_location=device)\n",
    "\n",
    "model = agent.model\n",
    "model.load_state_dict(params)\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "while not done:\n",
    "    action = agent.select_action(state)  # 使用最小探索率\n",
    "    next_observation, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = next_observation['agent'] / grid_world_size\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.3)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward:.2f}\")\n",
    "test_env.close()\n"
   ],
   "id": "d66c380f50445d0d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
