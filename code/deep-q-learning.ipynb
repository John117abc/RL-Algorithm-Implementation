{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用网格世界实现DQN算法",
   "id": "d57a8cffff14161b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T02:06:41.609142Z",
     "start_time": "2025-12-08T02:05:28.501792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "from torch import nn\n",
    "import random\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# 定义神经网络\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 设备选择\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size, obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 初始化参数\n",
    "replay_buffer = deque(maxlen=2000)  # 经验回放缓冲区\n",
    "batch_size = 64     # 批量大小\n",
    "update_frequency = 100  # 目标网络更新频率\n",
    "epsilon = 1.0       # 初始探索率\n",
    "epsilon_min = 0.01  # 最小探索率\n",
    "epsilon_decay = 0.995   # 探索率衰减\n",
    "gamma = 0.99        # 折扣因子\n",
    "lr = 0.001          # 学习率\n",
    "\n",
    "# 网络初始化 (输入维度=2，对应x,y坐标)\n",
    "main_network = NeuralNetwork(input_dim=2,hidden_dim=1024, output_dim=4).to(device)\n",
    "target_network = NeuralNetwork(input_dim=2, hidden_dim=1024,output_dim=4).to(device)\n",
    "target_network.load_state_dict(main_network.state_dict())\n",
    "target_network.eval()  # 目标网络不需要梯度\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(main_network.parameters(), lr=lr)\n",
    "\n",
    "# ε-Greedy动作选择\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions-1)\n",
    "\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = main_network(state_tensor)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# 训练函数\n",
    "def train():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # 随机采样批次\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    # 转换为张量\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "    # 计算当前Q值 (仅执行的动作)\n",
    "    current_q_values = main_network(states)\n",
    "    current_q_value = current_q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # 计算目标Q值 (使用目标网络)\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states)\n",
    "        max_next_q = next_q_values.max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * gamma * max_next_q\n",
    "\n",
    "    # 计算损失\n",
    "    loss = loss_fn(current_q_value.squeeze(), target_q_values)\n",
    "\n",
    "    # 优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# 训练循环\n",
    "global_step = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(1000):\n",
    "    # 重置环境\n",
    "    observation, _ = env.reset(seed=99,options={'enable_random_pos': True})\n",
    "    state = observation['agent'] / grid_world_size  # 归一化到[0,1]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < 100:  # 最大步数限制\n",
    "        # 选择动作\n",
    "        action = select_action(state, epsilon)\n",
    "\n",
    "        # 执行动作\n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = next_observation['agent'] / grid_world_size\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 存储经验\n",
    "        replay_buffer.append((state.copy(), action, reward, next_state.copy(), float(done)))\n",
    "\n",
    "        # 训练\n",
    "        train()\n",
    "\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        global_step += 1\n",
    "\n",
    "        # 定期更新目标网络\n",
    "        if global_step % update_frequency == 0:\n",
    "            target_network.load_state_dict(main_network.state_dict())\n",
    "\n",
    "    # 更新探索率\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    # 打印进度\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f\"回合 {episode}, 平均奖励: {avg_reward:.2f}, Epsilon: {epsilon:.4f}, 步数: {global_step}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(main_network.state_dict(), '../data/deep-q-learning/dqn-model.pth')"
   ],
   "id": "4cdaeba272859176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "回合 0, 平均奖励: -7.86, Epsilon: 0.9950, 步数: 100\n",
      "回合 10, 平均奖励: -0.22, Epsilon: 0.9464, 步数: 822\n",
      "回合 20, 平均奖励: -2.24, Epsilon: 0.9001, 步数: 1553\n",
      "回合 30, 平均奖励: -4.30, Epsilon: 0.8561, 步数: 2265\n",
      "回合 40, 平均奖励: -2.20, Epsilon: 0.8142, 步数: 2980\n",
      "回合 50, 平均奖励: 0.03, Epsilon: 0.7744, 步数: 3715\n",
      "回合 60, 平均奖励: 0.85, Epsilon: 0.7366, 步数: 4340\n",
      "回合 70, 平均奖励: -0.97, Epsilon: 0.7005, 步数: 5152\n",
      "回合 80, 平均奖励: 1.41, Epsilon: 0.6663, 步数: 5832\n",
      "回合 90, 平均奖励: -3.32, Epsilon: 0.6337, 步数: 6645\n",
      "回合 100, 平均奖励: -1.39, Epsilon: 0.6027, 步数: 7417\n",
      "回合 110, 平均奖励: -8.45, Epsilon: 0.5733, 步数: 8191\n",
      "回合 120, 平均奖励: 1.99, Epsilon: 0.5452, 步数: 8833\n",
      "回合 130, 平均奖励: 4.43, Epsilon: 0.5186, 步数: 9308\n",
      "回合 140, 平均奖励: 2.78, Epsilon: 0.4932, 步数: 9869\n",
      "回合 150, 平均奖励: 1.90, Epsilon: 0.4691, 步数: 10444\n",
      "回合 160, 平均奖励: 3.80, Epsilon: 0.4462, 步数: 10986\n",
      "回合 170, 平均奖励: 6.67, Epsilon: 0.4244, 步数: 11295\n",
      "回合 180, 平均奖励: 4.05, Epsilon: 0.4036, 步数: 11804\n",
      "回合 190, 平均奖励: 8.22, Epsilon: 0.3839, 步数: 12025\n",
      "回合 200, 平均奖励: 7.88, Epsilon: 0.3651, 步数: 12375\n",
      "回合 210, 平均奖励: 7.79, Epsilon: 0.3473, 步数: 12683\n",
      "回合 220, 平均奖励: 8.70, Epsilon: 0.3303, 步数: 12863\n",
      "回合 230, 平均奖励: 0.71, Epsilon: 0.3141, 步数: 13331\n",
      "回合 240, 平均奖励: 5.50, Epsilon: 0.2988, 步数: 13779\n",
      "回合 250, 平均奖励: 6.37, Epsilon: 0.2842, 步数: 14214\n",
      "回合 260, 平均奖励: 9.02, Epsilon: 0.2703, 步数: 14660\n",
      "回合 270, 平均奖励: 8.83, Epsilon: 0.2571, 步数: 14909\n",
      "回合 280, 平均奖励: 9.13, Epsilon: 0.2445, 步数: 15151\n",
      "回合 290, 平均奖励: 8.46, Epsilon: 0.2326, 步数: 15406\n",
      "回合 300, 平均奖励: 9.50, Epsilon: 0.2212, 步数: 15576\n",
      "回合 310, 平均奖励: 9.51, Epsilon: 0.2104, 步数: 15787\n",
      "回合 320, 平均奖励: 8.60, Epsilon: 0.2001, 步数: 16025\n",
      "回合 330, 平均奖励: 3.13, Epsilon: 0.1903, 步数: 16472\n",
      "回合 340, 平均奖励: 6.93, Epsilon: 0.1810, 步数: 16932\n",
      "回合 350, 平均奖励: 8.95, Epsilon: 0.1721, 步数: 17157\n",
      "回合 360, 平均奖励: 8.26, Epsilon: 0.1637, 步数: 17391\n",
      "回合 370, 平均奖励: 1.17, Epsilon: 0.1557, 步数: 17809\n",
      "回合 380, 平均奖励: 7.94, Epsilon: 0.1481, 步数: 18047\n",
      "回合 390, 平均奖励: 7.59, Epsilon: 0.1409, 步数: 18392\n",
      "回合 400, 平均奖励: 9.31, Epsilon: 0.1340, 步数: 18650\n",
      "回合 410, 平均奖励: 9.54, Epsilon: 0.1274, 步数: 18870\n",
      "回合 420, 平均奖励: 9.81, Epsilon: 0.1212, 步数: 19019\n",
      "回合 430, 平均奖励: 9.15, Epsilon: 0.1153, 步数: 19192\n",
      "回合 440, 平均奖励: 9.81, Epsilon: 0.1096, 步数: 19339\n",
      "回合 450, 平均奖励: 8.48, Epsilon: 0.1043, 步数: 19523\n",
      "回合 460, 平均奖励: 8.39, Epsilon: 0.0992, 步数: 19699\n",
      "回合 470, 平均奖励: 9.56, Epsilon: 0.0943, 步数: 19804\n",
      "回合 480, 平均奖励: 8.31, Epsilon: 0.0897, 步数: 20012\n",
      "回合 490, 平均奖励: 7.19, Epsilon: 0.0853, 步数: 20260\n",
      "回合 500, 平均奖励: 9.74, Epsilon: 0.0812, 步数: 20432\n",
      "回合 510, 平均奖励: 9.61, Epsilon: 0.0772, 步数: 20533\n",
      "回合 520, 平均奖励: 9.30, Epsilon: 0.0734, 步数: 20657\n",
      "回合 530, 平均奖励: 9.50, Epsilon: 0.0698, 步数: 20875\n",
      "回合 540, 平均奖励: 7.86, Epsilon: 0.0664, 步数: 21189\n",
      "回合 550, 平均奖励: 9.79, Epsilon: 0.0632, 步数: 21355\n",
      "回合 560, 平均奖励: 9.68, Epsilon: 0.0601, 步数: 21534\n",
      "回合 570, 平均奖励: 9.89, Epsilon: 0.0571, 步数: 21650\n",
      "回合 580, 平均奖励: 8.30, Epsilon: 0.0544, 步数: 21964\n",
      "回合 590, 平均奖励: 9.77, Epsilon: 0.0517, 步数: 22152\n",
      "回合 600, 平均奖励: 9.91, Epsilon: 0.0492, 步数: 22253\n",
      "回合 610, 平均奖励: 9.77, Epsilon: 0.0468, 步数: 22399\n",
      "回合 620, 平均奖励: 9.78, Epsilon: 0.0445, 步数: 22576\n",
      "回合 630, 平均奖励: 6.90, Epsilon: 0.0423, 步数: 22851\n",
      "回合 640, 平均奖励: 9.82, Epsilon: 0.0402, 步数: 22945\n",
      "回合 650, 平均奖励: 7.83, Epsilon: 0.0383, 步数: 23191\n",
      "回合 660, 平均奖励: 9.85, Epsilon: 0.0364, 步数: 23354\n",
      "回合 670, 平均奖励: 9.16, Epsilon: 0.0346, 步数: 23515\n",
      "回合 680, 平均奖励: 9.82, Epsilon: 0.0329, 步数: 23660\n",
      "回合 690, 平均奖励: 9.82, Epsilon: 0.0313, 步数: 23798\n",
      "回合 700, 平均奖励: 6.18, Epsilon: 0.0298, 步数: 23977\n",
      "回合 710, 平均奖励: 9.73, Epsilon: 0.0283, 步数: 24059\n",
      "回合 720, 平均奖励: 9.83, Epsilon: 0.0269, 步数: 24239\n",
      "回合 730, 平均奖励: 8.46, Epsilon: 0.0256, 步数: 24441\n",
      "回合 740, 平均奖励: 8.62, Epsilon: 0.0244, 步数: 24679\n",
      "回合 750, 平均奖励: 8.46, Epsilon: 0.0232, 步数: 24885\n",
      "回合 760, 平均奖励: 9.10, Epsilon: 0.0220, 步数: 25016\n",
      "回合 770, 平均奖励: 9.49, Epsilon: 0.0210, 步数: 25192\n",
      "回合 780, 平均奖励: 9.89, Epsilon: 0.0199, 步数: 25309\n",
      "回合 790, 平均奖励: 8.72, Epsilon: 0.0190, 步数: 25552\n",
      "回合 800, 平均奖励: 9.85, Epsilon: 0.0180, 步数: 25709\n",
      "回合 810, 平均奖励: 5.46, Epsilon: 0.0172, 步数: 26019\n",
      "回合 820, 平均奖励: 8.62, Epsilon: 0.0163, 步数: 26308\n",
      "回合 830, 平均奖励: 7.45, Epsilon: 0.0155, 步数: 26773\n",
      "回合 840, 平均奖励: 6.52, Epsilon: 0.0148, 步数: 27185\n",
      "回合 850, 平均奖励: 9.70, Epsilon: 0.0140, 步数: 27447\n",
      "回合 860, 平均奖励: 5.36, Epsilon: 0.0134, 步数: 27819\n",
      "回合 870, 平均奖励: 7.19, Epsilon: 0.0127, 步数: 28046\n",
      "回合 880, 平均奖励: 6.08, Epsilon: 0.0121, 步数: 28284\n",
      "回合 890, 平均奖励: 8.04, Epsilon: 0.0115, 步数: 28567\n",
      "回合 900, 平均奖励: 9.88, Epsilon: 0.0109, 步数: 28700\n",
      "回合 910, 平均奖励: 4.17, Epsilon: 0.0104, 步数: 29139\n",
      "回合 920, 平均奖励: 6.60, Epsilon: 0.0100, 步数: 29544\n",
      "回合 930, 平均奖励: 3.79, Epsilon: 0.0100, 步数: 30013\n",
      "回合 940, 平均奖励: 6.42, Epsilon: 0.0100, 步数: 30378\n",
      "回合 950, 平均奖励: 5.63, Epsilon: 0.0100, 步数: 30724\n",
      "回合 960, 平均奖励: 8.17, Epsilon: 0.0100, 步数: 30974\n",
      "回合 970, 平均奖励: 7.42, Epsilon: 0.0100, 步数: 31270\n",
      "回合 980, 平均奖励: 4.87, Epsilon: 0.0100, 步数: 31689\n",
      "回合 990, 平均奖励: 6.30, Epsilon: 0.0100, 步数: 32028\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用模型进行预测",
   "id": "f1d98025bf267eef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T02:07:02.766663Z",
     "start_time": "2025-12-08T02:06:59.453482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# 定义神经网络\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human', obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "\n",
    "epsilon_min = 0.01  # 最小探索率\n",
    "\n",
    "# 测试学习到的策略\n",
    "print(\"\\n正在用学习到的策略运行测试 episode...\")\n",
    "observation, _ = test_env.reset(seed=99,options={'enable_random_pos': True})\n",
    "state = observation['agent'] / grid_world_size\n",
    "done = False\n",
    "total_reward = 0\n",
    "# 设备选择\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"使用设备: {device}\")\n",
    "# ε-Greedy动作选择\n",
    "def select_action_test(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions-1)\n",
    "\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = predict_network(state_tensor)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# 加载模型参数\n",
    "model_path = '../data/deep-q-learning/dqn-model.pth'\n",
    "print(f\"正在从 {model_path} 加载模型...\")\n",
    "params = torch.load(model_path, map_location=device)\n",
    "\n",
    "predict_network = NeuralNetwork(input_dim=2,hidden_dim=1024, output_dim=4).to(device)\n",
    "predict_network.load_state_dict(params)\n",
    "predict_network.eval()  # 设置为评估模式\n",
    "\n",
    "while not done:\n",
    "    action = select_action_test(state, epsilon_min)  # 使用最小探索率\n",
    "    next_observation, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = next_observation['agent'] / grid_world_size\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.3)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward:.2f}\")\n",
    "test_env.close()\n"
   ],
   "id": "502f6567daa206f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在用学习到的策略运行测试 episode...\n",
      "使用设备: cuda\n",
      "正在从 ../data/deep-q-learning/dqn-model.pth 加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangchengxuan/anaconda3/envs/d2l/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试 episode 总奖励: 9.92\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
