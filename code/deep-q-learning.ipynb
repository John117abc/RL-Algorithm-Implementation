{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用网格世界实现DQN算法",
   "id": "d57a8cffff14161b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "from torch import nn\n",
    "import random\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# 定义神经网络\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 设备选择\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size, obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 初始化参数\n",
    "replay_buffer = deque(maxlen=2000)  # 经验回放缓冲区\n",
    "batch_size = 64     # 批量大小\n",
    "update_frequency = 100  # 目标网络更新频率\n",
    "epsilon = 1.0       # 初始探索率\n",
    "epsilon_min = 0.01  # 最小探索率\n",
    "epsilon_decay = 0.995   # 探索率衰减\n",
    "gamma = 0.99        # 折扣因子\n",
    "lr = 0.001          # 学习率\n",
    "\n",
    "# 网络初始化 (输入维度=2，对应x,y坐标)\n",
    "main_network = NeuralNetwork(input_dim=2,hidden_dim=1024, output_dim=4).to(device)\n",
    "target_network = NeuralNetwork(input_dim=2, hidden_dim=1024,output_dim=4).to(device)\n",
    "target_network.load_state_dict(main_network.state_dict())\n",
    "target_network.eval()  # 目标网络不需要梯度\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(main_network.parameters(), lr=lr)\n",
    "\n",
    "# ε-Greedy动作选择\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions-1)\n",
    "\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = main_network(state_tensor)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# 训练函数\n",
    "def train():\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # 随机采样批次\n",
    "    batch = random.sample(replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    # 转换为张量\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "    # 计算当前Q值 (仅执行的动作)\n",
    "    current_q_values = main_network(states)\n",
    "    current_q_value = current_q_values.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    # 计算目标Q值 (使用目标网络)\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_network(next_states)\n",
    "        max_next_q = next_q_values.max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * gamma * max_next_q\n",
    "\n",
    "    # 计算损失\n",
    "    loss = loss_fn(current_q_value.squeeze(), target_q_values)\n",
    "\n",
    "    # 优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# 训练循环\n",
    "global_step = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(1000):\n",
    "    # 重置环境\n",
    "    observation, _ = env.reset(seed=99,options={'enable_random_pos': True})\n",
    "    state = observation['agent'] / grid_world_size  # 归一化到[0,1]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < 100:  # 最大步数限制\n",
    "        # 选择动作\n",
    "        action = select_action(state, epsilon)\n",
    "\n",
    "        # 执行动作\n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = next_observation['agent'] / grid_world_size\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 存储经验\n",
    "        replay_buffer.append((state.copy(), action, reward, next_state.copy(), float(done)))\n",
    "\n",
    "        # 训练\n",
    "        train()\n",
    "\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        global_step += 1\n",
    "\n",
    "        # 定期更新目标网络\n",
    "        if global_step % update_frequency == 0:\n",
    "            target_network.load_state_dict(main_network.state_dict())\n",
    "\n",
    "    # 更新探索率\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    # 打印进度\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f\"回合 {episode}, 平均奖励: {avg_reward:.2f}, Epsilon: {epsilon:.4f}, 步数: {global_step}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(main_network.state_dict(), '../data/deep-q-learning/dqn-model.pth')"
   ],
   "id": "4cdaeba272859176",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用模型进行预测",
   "id": "f1d98025bf267eef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# 定义神经网络\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human', obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "\n",
    "epsilon_min = 0.01  # 最小探索率\n",
    "\n",
    "# 测试学习到的策略\n",
    "print(\"\\n正在用学习到的策略运行测试 episode...\")\n",
    "observation, _ = test_env.reset(seed=99,options={'enable_random_pos': True})\n",
    "state = observation['agent'] / grid_world_size\n",
    "done = False\n",
    "total_reward = 0\n",
    "# 设备选择\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"使用设备: {device}\")\n",
    "# ε-Greedy动作选择\n",
    "def select_action_test(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions-1)\n",
    "\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = predict_network(state_tensor)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# 加载模型参数\n",
    "model_path = '../data/deep-q-learning/dqn-model.pth'\n",
    "print(f\"正在从 {model_path} 加载模型...\")\n",
    "params = torch.load(model_path, map_location=device)\n",
    "\n",
    "predict_network = NeuralNetwork(input_dim=2,hidden_dim=1024, output_dim=4).to(device)\n",
    "predict_network.load_state_dict(params)\n",
    "predict_network.eval()  # 设置为评估模式\n",
    "\n",
    "while not done:\n",
    "    action = select_action_test(state, epsilon_min)  # 使用最小探索率\n",
    "    next_observation, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = next_observation['agent'] / grid_world_size\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.3)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward:.2f}\")\n",
    "test_env.close()\n"
   ],
   "id": "502f6567daa206f5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
