{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 动作值估计Sarsa",
   "id": "199b89919a96f96c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 200\n",
    "    episode = []\n",
    "    while not done and episode_max >0:\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "        # sarsa的核心步骤\n",
    "        next_action = epsilon_greedy_action(q_table, next_state, epsilon, n_actions)\n",
    "        q_table[state][action] += learning_rate * (reward + gamma * q_table[next_state][next_action] - q_table[state][action])\n",
    "        action = next_action\n",
    "\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max -= 1\n",
    "\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "455eb08b994f7519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Expected Sarsa算法",
   "id": "c21dc34a59d9775c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 下一步所有可能得动作期望\n",
    "def compute_expected_q(q_table, next_state, epsilon, n_actions):\n",
    "    max_action = np.argmax(q_table[next_state])\n",
    "    max_action_probability = 1.0 - epsilon + (epsilon / n_actions)\n",
    "    other_actions_probability = (1.0 - max_action_probability) / (n_actions - 1) if n_actions > 1 else 1.0\n",
    "\n",
    "    other_score = 0.0\n",
    "    for a in range(n_actions):\n",
    "        if a != max_action:\n",
    "            other_score += other_actions_probability * q_table[next_state][a]\n",
    "\n",
    "    return max_action_probability * q_table[next_state][max_action] + other_score\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 200\n",
    "    episode = []\n",
    "    while not done and episode_max >0:\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "        # Expected Sarsa的核心步骤\n",
    "        next_action = epsilon_greedy_action(q_table, next_state, epsilon, n_actions)\n",
    "        expected = compute_expected_q(q_table,next_state, epsilon, n_actions)\n",
    "        q_table[state][action] += learning_rate * (reward + gamma * expected - q_table[state][action])\n",
    "        action = next_action\n",
    "\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max -= 1\n",
    "\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "20bc65fa75912eef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### n-step Sarsa算法",
   "id": "200ac4572531e98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 20\n",
    "learning_rate = 0.01\n",
    "n_step = 10\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 0\n",
    "    episode = []\n",
    "    while not done and episode_max < 200:\n",
    "\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "\n",
    "        # n-step Sarsa核心步骤\n",
    "        G = 0.0\n",
    "        G_count = 10\n",
    "        if episode_max > n_step:\n",
    "            n_step,n_action,n_reward = episode[episode_max - n_step]\n",
    "            for t in reversed(range(len(episode))):\n",
    "                s, a, r = episode[t]\n",
    "                G += r + gamma * G\n",
    "                G_count -= 1\n",
    "                if G_count == 1:\n",
    "                    break\n",
    "            # q-learning的核心步骤\n",
    "            q_table[n_step][n_action] = q_table[n_step][n_action] + learning_rate *[G - q_table[n_step][n_action]]\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max += 1\n",
    "\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "ab61d8982077b3b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 最优动作值估计Q-learning",
   "id": "b61c07a7dc55a190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 200\n",
    "    episode = []\n",
    "    while not done and episode_max >0:\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "        # q-learning的核心步骤\n",
    "        q_table[state][action] += learning_rate * (reward + gamma * q_table[next_state][np.argmax(q_table[next_state])] - q_table[state][action])\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max -= 1\n",
    "\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "b83d5a8e62e78c35",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
