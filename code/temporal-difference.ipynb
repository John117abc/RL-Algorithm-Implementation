{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 动作值估计Sarsa",
   "id": "199b89919a96f96c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 200\n",
    "    episode = []\n",
    "    while not done and episode_max >0:\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "        # sarsa的核心步骤\n",
    "        next_action = epsilon_greedy_action(q_table, next_state, epsilon, n_actions)\n",
    "        q_table[state][action] += learning_rate * (reward + gamma * q_table[next_state][next_action] - q_table[state][action])\n",
    "        action = next_action\n",
    "\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max -= 1\n",
    "\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "455eb08b994f7519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Expected Sarsa算法",
   "id": "c21dc34a59d9775c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 下一步所有可能得动作期望\n",
    "def compute_expected_q(q_table, next_state, epsilon, n_actions):\n",
    "    max_action = np.argmax(q_table[next_state])\n",
    "    max_action_probability = 1.0 - epsilon + (epsilon / n_actions)\n",
    "    other_actions_probability = (1.0 - max_action_probability) / (n_actions - 1) if n_actions > 1 else 1.0\n",
    "\n",
    "    other_score = 0.0\n",
    "    for a in range(n_actions):\n",
    "        if a != max_action:\n",
    "            other_score += other_actions_probability * q_table[next_state][a]\n",
    "\n",
    "    return max_action_probability * q_table[next_state][max_action] + other_score\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 200\n",
    "    episode = []\n",
    "    while not done and episode_max >0:\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "        # Expected Sarsa的核心步骤\n",
    "        next_action = epsilon_greedy_action(q_table, next_state, epsilon, n_actions)\n",
    "        expected = compute_expected_q(q_table,next_state, epsilon, n_actions)\n",
    "        q_table[state][action] += learning_rate * (reward + gamma * expected - q_table[state][action])\n",
    "        action = next_action\n",
    "\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max -= 1\n",
    "\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "20bc65fa75912eef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### n-step Sarsa算法",
   "id": "200ac4572531e98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T09:14:40.154692Z",
     "start_time": "2025-11-27T09:14:28.911162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 98\n",
    "learning_rate = 0.01\n",
    "n_step = 10\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 0\n",
    "    episode = []\n",
    "    while not done and episode_max < 200:\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "        # n-step Sarsa核心步骤\n",
    "        G = 0.0\n",
    "        tau = len(episode) - n_step - 1\n",
    "        if episode_max > n_step:\n",
    "            for t in range(n_step):\n",
    "                s, a, r = episode[tau + t]\n",
    "                G += (gamma ** t) * r\n",
    "            G +=  (gamma ** n_step) * q_table[state][action]\n",
    "            before_step,before_action,before_reward = episode[tau]\n",
    "            q_table[before_step][before_action] = q_table[before_step][before_action] + learning_rate *(G - q_table[before_step][before_action])\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max += 1\n",
    "\n",
    "    # n-step Sarsa核心步骤更新尾部信息\n",
    "    T = len(episode)\n",
    "    for tau in range(max(0, T - n_step), T):\n",
    "        s_tau, a_tau, _ = episode[tau]\n",
    "        G = 0.0\n",
    "        for i in range(tau, T):\n",
    "            G += (gamma ** (i - tau)) * episode[i][2]\n",
    "        q_table[s_tau][a_tau] += learning_rate * (G - q_table[s_tau][a_tau])\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed = seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "ab61d8982077b3b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, epsilon: 0.9995\n",
      "Episode 100, epsilon: 0.9507\n",
      "Episode 200, epsilon: 0.9044\n",
      "Episode 300, epsilon: 0.8602\n",
      "Episode 400, epsilon: 0.8183\n",
      "Episode 500, epsilon: 0.7784\n",
      "Episode 600, epsilon: 0.7404\n",
      "Episode 700, epsilon: 0.7043\n",
      "Episode 800, epsilon: 0.6699\n",
      "Episode 900, epsilon: 0.6372\n",
      "Episode 1000, epsilon: 0.6062\n",
      "Episode 1100, epsilon: 0.5766\n",
      "Episode 1200, epsilon: 0.5485\n",
      "Episode 1300, epsilon: 0.5217\n",
      "Episode 1400, epsilon: 0.4963\n",
      "Episode 1500, epsilon: 0.4720\n",
      "Episode 1600, epsilon: 0.4490\n",
      "Episode 1700, epsilon: 0.4271\n",
      "Episode 1800, epsilon: 0.4063\n",
      "Episode 1900, epsilon: 0.3865\n",
      "Episode 2000, epsilon: 0.3676\n",
      "Episode 2100, epsilon: 0.3497\n",
      "Episode 2200, epsilon: 0.3326\n",
      "Episode 2300, epsilon: 0.3164\n",
      "Episode 2400, epsilon: 0.3010\n",
      "Episode 2500, epsilon: 0.2863\n",
      "Episode 2600, epsilon: 0.2723\n",
      "Episode 2700, epsilon: 0.2590\n",
      "Episode 2800, epsilon: 0.2464\n",
      "Episode 2900, epsilon: 0.2344\n",
      "Episode 3000, epsilon: 0.2229\n",
      "Episode 3100, epsilon: 0.2121\n",
      "Episode 3200, epsilon: 0.2017\n",
      "Episode 3300, epsilon: 0.1919\n",
      "Episode 3400, epsilon: 0.1825\n",
      "Episode 3500, epsilon: 0.1736\n",
      "Episode 3600, epsilon: 0.1651\n",
      "Episode 3700, epsilon: 0.1571\n",
      "Episode 3800, epsilon: 0.1494\n",
      "Episode 3900, epsilon: 0.1421\n",
      "Episode 4000, epsilon: 0.1352\n",
      "Episode 4100, epsilon: 0.1286\n",
      "Episode 4200, epsilon: 0.1223\n",
      "Episode 4300, epsilon: 0.1164\n",
      "Episode 4400, epsilon: 0.1107\n",
      "Episode 4500, epsilon: 0.1053\n",
      "Episode 4600, epsilon: 0.1002\n",
      "Episode 4700, epsilon: 0.0953\n",
      "Episode 4800, epsilon: 0.0906\n",
      "Episode 4900, epsilon: 0.0862\n",
      "Episode 5000, epsilon: 0.0820\n",
      "Episode 5100, epsilon: 0.0780\n",
      "Episode 5200, epsilon: 0.0742\n",
      "Episode 5300, epsilon: 0.0706\n",
      "Episode 5400, epsilon: 0.0671\n",
      "Episode 5500, epsilon: 0.0639\n",
      "Episode 5600, epsilon: 0.0607\n",
      "Episode 5700, epsilon: 0.0578\n",
      "Episode 5800, epsilon: 0.0550\n",
      "Episode 5900, epsilon: 0.0523\n",
      "Episode 6000, epsilon: 0.0497\n",
      "Episode 6100, epsilon: 0.0473\n",
      "Episode 6200, epsilon: 0.0450\n",
      "Episode 6300, epsilon: 0.0428\n",
      "Episode 6400, epsilon: 0.0407\n",
      "Episode 6500, epsilon: 0.0387\n",
      "Episode 6600, epsilon: 0.0368\n",
      "Episode 6700, epsilon: 0.0350\n",
      "Episode 6800, epsilon: 0.0333\n",
      "Episode 6900, epsilon: 0.0317\n",
      "Episode 7000, epsilon: 0.0302\n",
      "Episode 7100, epsilon: 0.0287\n",
      "Episode 7200, epsilon: 0.0273\n",
      "Episode 7300, epsilon: 0.0260\n",
      "Episode 7400, epsilon: 0.0247\n",
      "Episode 7500, epsilon: 0.0235\n",
      "Episode 7600, epsilon: 0.0223\n",
      "Episode 7700, epsilon: 0.0212\n",
      "Episode 7800, epsilon: 0.0202\n",
      "Episode 7900, epsilon: 0.0192\n",
      "Episode 8000, epsilon: 0.0183\n",
      "Episode 8100, epsilon: 0.0174\n",
      "Episode 8200, epsilon: 0.0165\n",
      "Episode 8300, epsilon: 0.0157\n",
      "Episode 8400, epsilon: 0.0150\n",
      "Episode 8500, epsilon: 0.0142\n",
      "Episode 8600, epsilon: 0.0135\n",
      "Episode 8700, epsilon: 0.0129\n",
      "Episode 8800, epsilon: 0.0123\n",
      "Episode 8900, epsilon: 0.0117\n",
      "Episode 9000, epsilon: 0.0111\n",
      "Episode 9100, epsilon: 0.0105\n",
      "Episode 9200, epsilon: 0.0100\n",
      "Episode 9300, epsilon: 0.0100\n",
      "Episode 9400, epsilon: 0.0100\n",
      "Episode 9500, epsilon: 0.0100\n",
      "Episode 9600, epsilon: 0.0100\n",
      "Episode 9700, epsilon: 0.0100\n",
      "Episode 9800, epsilon: 0.0100\n",
      "Episode 9900, epsilon: 0.0100\n",
      "\n",
      "正在用新策略运行一个可视化 episode...\n",
      "测试 episode 总奖励: 6\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 最优动作值估计Q-learning",
   "id": "b61c07a7dc55a190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from rl_utils.GNGridWorldEnv import GridWorldEnv\n",
    "\n",
    "# ε-Greedy策略生成\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(0, n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# 设置环境\n",
    "grid_world_size = 10\n",
    "obstacle_count = 20\n",
    "env = GridWorldEnv(size=grid_world_size,obstacle_count=obstacle_count)\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((grid_world_size * grid_world_size, n_actions))\n",
    "\n",
    "env.render()  # 显示窗口\n",
    "\n",
    "\n",
    "# 初始参数\n",
    "gamma = 0.95\n",
    "epsilon = 1.0   # 随机epsilon，强探索\n",
    "epsilon_decay = 0.9995   # eps的每轮衰减率\n",
    "epsilon_min = 0.01 # 最小的eps\n",
    "max_iterate = 10000\n",
    "seed = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "state_count = np.zeros((grid_world_size * grid_world_size, n_actions))  # 访问次数\n",
    "for it_index in range(max_iterate):\n",
    "\n",
    "    observation, _ = env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "    state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "    done = False\n",
    "    episode_max = 200\n",
    "    episode = []\n",
    "    while not done and episode_max >0:\n",
    "        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "        episode.append((state, action, reward))\n",
    "        # q-learning的核心步骤\n",
    "        q_table[state][action] += learning_rate * (reward + gamma * q_table[next_state][np.argmax(q_table[next_state])] - q_table[state][action])\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        episode_max -= 1\n",
    "\n",
    "    # 更新epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay,epsilon_min)\n",
    "    # 打印进度\n",
    "    if it_index % 100 == 0:\n",
    "        print(f\"Episode {it_index}, epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n正在用新策略运行一个可视化 episode...\")\n",
    "test_env = GridWorldEnv(size=grid_world_size, render_mode='human',obstacle_count=obstacle_count)\n",
    "n_actions = test_env.action_space.n\n",
    "observation, _ = test_env.reset(seed=seed, options={'enable_random_pos': True})\n",
    "state = int((observation['agent'][0] * grid_world_size) + observation['agent'][1])\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    state = int((next_state['agent'][0] * grid_world_size) + next_state['agent'][1])\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"测试 episode 总奖励: {total_reward}\")\n",
    "test_env.close()\n",
    "env.close()"
   ],
   "id": "b83d5a8e62e78c35",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
